# User stories for objectDetection add-on

1. [x] To understand an image and it's contents, as a blind or low-vision user I can trigger object detection on an image on my screen using a gesture.
    1. [x] To understand what objects have been identified in an image, I can trigger a command using a gesture to announce all of the detected objects.
    2. [x] So that I can consume long results letter-by-letter or word-by-word, or easily copy the result, I can trigger a version of the command to present the objects in a virtual window and use navigational gestures to traverse the result without moving the system focus.

2. [ ] So that I can avoid waiting for results from an object detection process mistakenly run on non-image objects, as a blind user I can run a version of the command that filters out non-image elements.

3. [ ] To run object detection on a objects that are not identified by NVDA as graphics, as a low-vision user I can identify these objects as images and trigger a version of the command to perform object-detection irrespective of the type of object.

4. [ ] To understand the relative positions of the detected objects in an image, as a blind or low-vision user, I can trigger a command using gestures to draw bounding boxes over the detected objects.
    1. [ ] To understand the object enclosed by a bounding box, as a blind or low-vision user I can move the mouse pointer over or touch the bounding box to announce the object label.

5. [ ] To access the previous object detection result after mistakenly escaping, I can trigger similar commands to be presented with the previous result.